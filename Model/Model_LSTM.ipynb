{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM & Word2Vec\n",
        "โค้ดส่วนนี้ปรับมาจาก https://kobkrit.com/%E0%B9%81%E0%B8%88%E0%B8%81%E0%B9%84%E0%B8%9A%E0%B9%80%E0%B8%9A%E0%B8%B4%E0%B9%89%E0%B8%A5-%E0%B8%A7%E0%B8%B4%E0%B8%98%E0%B8%B5%E0%B8%81%E0%B8%B2%E0%B8%A3-finetune-bert-roberta-wangchanberta-%E0%B8%AA%E0%B8%B3%E0%B8%AB%E0%B8%A3%E0%B8%B1%E0%B8%9A%E0%B8%87%E0%B8%B2%E0%B8%99-nlp-%E0%B8%A0%E0%B8%B2%E0%B8%A9%E0%B8%B2%E0%B9%84%E0%B8%97%E0%B8%A2%E0%B9%81%E0%B8%9A%E0%B8%9A%E0%B8%87%E0%B9%88%E0%B8%B2%E0%B8%A2-1fbbaac7c905"
      ],
      "metadata": {
        "id": "LoGj1gyAAdMV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j5rRUHhyZ_F"
      },
      "source": [
        "## Preparing VM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p2JBByAyYvw"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67ccu9bkyeMQ"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuJKXyi0jRuH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8Nm8S3L2e09"
      },
      "outputs": [],
      "source": [
        "# Command line tool for monitoring VM\n",
        "! apt-get -qq install htop -y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#นำเข้าข้อมูล"
      ],
      "metadata": {
        "id": "ed1vyBMG-HsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train= pd.read_csv('/content/train.csv')\n",
        "test= pd.read_csv('/content/test.csv')"
      ],
      "metadata": {
        "id": "1SMgxMB--HQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "upbj421I-SUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "R5NzdQ_D-Wxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejJ6P_tJk28U"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unXSVKLjJDRJ"
      },
      "outputs": [],
      "source": [
        "!pip install pythainlp python-crfsuite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjO58Y8i__Ww"
      },
      "outputs": [],
      "source": [
        "from pythainlp import word_vector\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmJRacBshoXl"
      },
      "outputs": [],
      "source": [
        "wv = word_vector.WordVector()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRHUcFzfhyBr"
      },
      "outputs": [],
      "source": [
        "####  Word to Vec object\n",
        "word2vec = wv.get_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJh-8LM0AG-h"
      },
      "outputs": [],
      "source": [
        "def get_average_vector(cleaned_query):\n",
        "    sum_query = None\n",
        "    count=0\n",
        "    for query in cleaned_query:\n",
        "        try:\n",
        "            if (sum_query is None):\n",
        "                sum_query = np.copy(word2vec[query])\n",
        "            else:\n",
        "                sum_query += word2vec[query]\n",
        "            count+=1\n",
        "            print(\"Word \"+query+ \" found!\")\n",
        "        except:\n",
        "            print(\"Word \"+query+\" is not found!\")\n",
        "    if (sum_query is not None):\n",
        "        average_query = sum_query/count\n",
        "    else:\n",
        "        average_query = np.zeros((300,))\n",
        "    return average_query\n",
        "\n",
        "average_query = get_average_vector([\"ทดสอบ\",\"ดู\",\"นะ\",\"ครับ\"])\n",
        "print(average_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0akAOkzUAfRx"
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "\n",
        "# Make average vectors of each class\n",
        "avg_w = {}\n",
        "for i in range(3):\n",
        "  avg_w[i] = reduce(lambda x,y: x+y, train[train[\"sentiment\"]==i][:100]['wordseged_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vK3GmDuhBWgW"
      },
      "outputs": [],
      "source": [
        "avg_v = {}\n",
        "for i in range(3):\n",
        "  print(i, len(avg_w[i]))\n",
        "  avg_v[i] = get_average_vector(avg_w[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMqZ1MofASqR"
      },
      "outputs": [],
      "source": [
        "def cosine_sim(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uvr0s2iAWva"
      },
      "outputs": [],
      "source": [
        "qv = get_average_vector(['ประธาน', 'ไม่', 'แข็งแรง', 'พอ', 'คือ', 'ขี้กลัว', 'ทำไม', 'ไม่', 'ใช้อำนาจ', 'ประธาน', 'สภา', 'เลย', 'โหวต', 'ทำไม'])\n",
        "qv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-PAyhiID9qA"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    print(i, cosine_sim(qv, avg_v[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM8qebQ_6qxs"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBPU0CMJI9ZI"
      },
      "outputs": [],
      "source": [
        "! pip install pythainlp python-crfsuite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWNczwg2xFXC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "train_t = train.copy()\n",
        "temp = train_t.wordseged_text.apply(lambda x : len(x)).values\n",
        "plt.hist(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj_ouq6VEx_W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
        "#from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.initializers import glorot_uniform\n",
        "np.random.seed(23)\n",
        "from pythainlp import word_vector\n",
        "import numpy as np\n",
        "#word2vec = word_vector.get_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikw-1CrOHy-4"
      },
      "outputs": [],
      "source": [
        "print(\"Vocab in Thai2Vec:\",len(word2vec.key_to_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMJevES2kGgv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sentences_to_indices(X, word2vec, max_len):\n",
        "    \"\"\"\n",
        "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
        "    The output shape should be such that it can be given to `Embedding()`.\n",
        "\n",
        "    Arguments:\n",
        "    X -- array of sentences (strings), of shape (m, 1)\n",
        "    word2vec -- a trained Word2Vec model from gensim\n",
        "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this.\n",
        "\n",
        "    Returns:\n",
        "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[0]                                   # number of training examples\n",
        "\n",
        "    # Initialize X_indices as a numpy matrix of zeros and the correct shape\n",
        "    X_indices = np.zeros((m, max_len))\n",
        "\n",
        "    for i in range(m):                               # loop over training examples\n",
        "\n",
        "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
        "        # print(X)\n",
        "        # print(len(X[i].lower().split()))\n",
        "        sentence_words = X[i].lower().split()[:max_len]\n",
        "\n",
        "        # Initialize j to 0\n",
        "        j = 0\n",
        "        try:\n",
        "        # Loop over the words of sentence_words\n",
        "          for w in sentence_words:\n",
        "              # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "\n",
        "                if w in word2vec.key_to_index:\n",
        "                    X_indices[i, j] = word2vec.key_to_index[w]\n",
        "                    # Increment j to j + 1\n",
        "                    j += 1\n",
        "        except:\n",
        "              print('key error: ', w)\n",
        "\n",
        "\n",
        "    return X_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCvQsZnMzY9Q"
      },
      "outputs": [],
      "source": [
        "'ฏ' in word2vec.key_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNOdu8EnLd5B"
      },
      "outputs": [],
      "source": [
        "X1 = np.array([\"สวัสดี ครับ ผม\", \"ทดสอบ ทดสอบ ทดสอบ\", \"เรา มา ทดสอบ กัน\",\"8\"])\n",
        "X1_indices = sentences_to_indices(X1, word2vec, max_len = 10)\n",
        "print(\"X1 =\", X1)\n",
        "print(\"X1_indices =\", X1_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwATeYZROZNC"
      },
      "outputs": [],
      "source": [
        "def convert_to_one_hot(y, maxLen):\n",
        "    max_value = max(y)\n",
        "    min_value = min(y)\n",
        "    length = len(y)\n",
        "    one_hot = np.zeros((length, (max_value - min_value + 1)))\n",
        "    one_hot[np.arange(length), y] = 1\n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE07VTZhk_7n"
      },
      "outputs": [],
      "source": [
        "word2vec[\"สวัสดี\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5q7gFaQlFdi"
      },
      "outputs": [],
      "source": [
        " word2vec.key_to_index.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlWFj4GllL6S"
      },
      "outputs": [],
      "source": [
        "word2vec[\"ประชาชน\"].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE1QkyhxlRUR"
      },
      "outputs": [],
      "source": [
        "vocab_len = len(word2vec.key_to_index) + 1\n",
        "emb_dim = word2vec[\"สวัสดี\"].shape[0]\n",
        "emb_matrix = np.zeros((vocab_len, emb_dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvjpWuVgljZv"
      },
      "outputs": [],
      "source": [
        "for word, index in word2vec.key_to_index.items():\n",
        "      #print(word, index)\n",
        "      emb_matrix[index, :] = word2vec[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luUOVC5aFzSK"
      },
      "outputs": [],
      "source": [
        "def pretrained_embedding_layer(word2vec):\n",
        "    \"\"\"\n",
        "    Creates a Keras Embedding() layer and loads in pre-trained PyThaiNLP word_vector 300-dimensional vectors.\n",
        "\n",
        "    Arguments:\n",
        "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    embedding_layer -- pretrained layer Keras instance\n",
        "    \"\"\"\n",
        "\n",
        "    vocab_len = len(word2vec.key_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
        "    emb_dim = word2vec[\"ประชาชน\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
        "\n",
        "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
        "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
        "\n",
        "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
        "    for word, index in word2vec.key_to_index.items():\n",
        "        # print(word)\n",
        "        emb_matrix[index, :] = word2vec[word]\n",
        "\n",
        "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False.\n",
        "    embedding_layer = Embedding(vocab_len, emb_dim)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
        "    embedding_layer.build((None,))\n",
        "\n",
        "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
        "    embedding_layer.set_weights([emb_matrix])\n",
        "\n",
        "    return embedding_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqz7RAKFHc8c"
      },
      "outputs": [],
      "source": [
        "embedding_layer = pretrained_embedding_layer(word2vec)\n",
        "print(\"weights[0][1] =\", embedding_layer.get_weights()[0][1])\n",
        "# print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H35GKAIJ-4d"
      },
      "outputs": [],
      "source": [
        "def text_clasifier_lstm(input_shape, word2vec):\n",
        "    \"\"\"\n",
        "    Function creating the Emojify-v2 model's graph.\n",
        "\n",
        "    Arguments:\n",
        "    input_shape -- shape of the input, usually (max_len,)\n",
        "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    model -- a model instance in Keras\n",
        "    \"\"\"\n",
        "\n",
        "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
        "    sentence_indices = Input(shape=input_shape, dtype=np.int32)\n",
        "\n",
        "    # Create the embedding layer pretrained with Word2Vec Vectors\n",
        "    embedding_layer =  pretrained_embedding_layer(word2vec)\n",
        "\n",
        "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
        "    embeddings = embedding_layer(sentence_indices)\n",
        "\n",
        "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
        "    # Be careful, the returned output should be a batch of sequences.\n",
        "    X = LSTM(128, return_sequences=True)(embeddings)\n",
        "    # Add dropout with a probability of 0.1\n",
        "    X = Dropout(0.1)(X)\n",
        "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
        "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
        "    X = LSTM(128)(X)\n",
        "    # Add dropout with a probability of 0.1\n",
        "    X = Dropout(0.1)(X)\n",
        "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
        "    X = Dense(3, activation='softmax')(X)\n",
        "    # Add a softmax activation\n",
        "    X = Activation('softmax')(X)\n",
        "\n",
        "    # Create Model instance which converts sentence_indices into X.\n",
        "    model = Model(sentence_indices, X)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGcZTsdxK1Ui"
      },
      "outputs": [],
      "source": [
        "model = text_clasifier_lstm((128,), word2vec)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AoxuIKe2tWU"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR7Uo_RV3A3W"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Student_Project/Vote_check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owcFLz6yDAIJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "model = text_clasifier_lstm((128,), word2vec)\n",
        "model.summary()\n",
        "# optimizer = optim.Adam(first_model.parameters(),lr=0.1e-3,weight_decay=1e-3)\n",
        "model.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=['accuracy'])\n",
        "X_train_indices = sentences_to_indices(train['wordseged_space_text'].values, word2vec, 128)\n",
        "Y_train_oh = convert_to_one_hot((train['sentiment']), 3)\n",
        "# ถ้า X_train_indices และ Y_train_oh เป็นข้อมูลการฝึกของคุณ\n",
        "history = model.fit(X_train_indices, Y_train_oh, epochs=500, batch_size=32, shuffle=True, validation_split=0.2)\n",
        "# บันทึกโมเดลลง disk\n",
        "model.save('my_model4.h5')\n",
        "\n",
        "#filehandler = open('model4.pickle', 'wb')\n",
        "#pickle.dump(model, filehandler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otldRdLgL1DS"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "filehandler = open('history.pickle', 'wb')\n",
        "pickle.dump(history, filehandler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krosqx2TMtub"
      },
      "outputs": [],
      "source": [
        "filehandler = open('history.pickle', 'rb')\n",
        "loaded_history = pickle.load(filehandler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dJ0pmgZH-Ir"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def plot_learningCurve(histoty):\n",
        "\n",
        "  # Extract loss and accuracy values from the training history\n",
        "  train_loss_values = histoty.history['loss']\n",
        "  train_accuracy_values = histoty.history['accuracy']\n",
        "  val_loss_values = histoty.history['val_loss']\n",
        "  val_accuracy_values = histoty.history['val_accuracy']\n",
        "\n",
        "  # Plot training and validation loss\n",
        "  plt.figure(figsize=(8, 8))\n",
        "\n",
        "  # plt.subplot(1, 2, 1)\n",
        "  plt.plot(train_loss_values, label='Training Loss')\n",
        "  plt.plot(val_loss_values, label='Validation Loss')\n",
        "  plt.title('Model Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  # Plot training and validation accuracy\n",
        "  # plt.subplot(1, 2, 2)\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.plot(train_accuracy_values, label='Training Accuracy')\n",
        "  plt.plot(val_accuracy_values, label='Validation Accuracy')\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def compute_Confusion(test,model):\n",
        "\n",
        "  test2 = np.array(test['wordseged_space_text'])\n",
        "  # Assuming you have test data\n",
        "  X_test_indices = sentences_to_indices(test2, word2vec, 128)\n",
        "\n",
        "  # One-hot encode the 'sentiment' column\n",
        "  Y_test_oh = pd.get_dummies(test['sentiment']).values\n",
        "\n",
        "  # Make predictions on the test set\n",
        "  Y_pred = model.predict(X_test_indices)\n",
        "\n",
        "  # Convert one-hot encoded predictions and true labels back to class labels\n",
        "  Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
        "  Y_true_classes = np.argmax(Y_test_oh, axis=1)\n",
        "  rp02 =( Y_true_classes == 0) & (Y_pred_classes == 2)\n",
        "  #print(test2[rp02])\n",
        "  # Confusion Matrix\n",
        "  conf_matrix = confusion_matrix(Y_true_classes, Y_pred_classes)\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(conf_matrix)\n",
        "\n",
        "  # Classification Report with Precision, Recall, and F1-Score\n",
        "  class_report = classification_report(Y_true_classes, Y_pred_classes)\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(class_report)\n",
        "\n",
        "compute_Confusion(test, model)\n",
        "plot_learningCurve(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0YfpOwmwrr9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract loss and accuracy values from the training history\n",
        "train_loss_values = history.history['loss']\n",
        "train_accuracy_values = history.history['accuracy']\n",
        "val_loss_values = history.history['val_loss']\n",
        "val_accuracy_values = history.history['val_accuracy']\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss_values, label='Training Loss')\n",
        "plt.plot(val_loss_values, label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# Plot training and validation accuracy\n",
        "# plt.subplot(1, 2, 2)\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(train_accuracy_values, label='Training Accuracy')\n",
        "plt.plot(val_accuracy_values, label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXO0lDFjVi4c"
      },
      "outputs": [],
      "source": [
        "# Extract loss and accuracy values from the training history\n",
        "train_loss_values = history.history['loss']\n",
        "train_accuracy_values = history.history['accuracy']\n",
        "val_loss_values = history.history['val_loss']\n",
        "val_accuracy_values = history.history['val_accuracy']\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(train_loss_values)\n",
        "plt.title('Model Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# Plot training accuracy\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(val_loss_values)\n",
        "plt.title('Model Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gThGLX5wxvG0"
      },
      "outputs": [],
      "source": [
        "X_train_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJyxB_tBSrY1"
      },
      "outputs": [],
      "source": [
        "preds = model.predict(X_train_indices[:10])\n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lbdrrRX3Rbu"
      },
      "outputs": [],
      "source": [
        "word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-f6u5dqxm3X"
      },
      "outputs": [],
      "source": [
        "test2 = np.array(test['wordseged_space_text'][101:102])\n",
        "sentences_to_indices(test2, word2vec, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R7TtgROm9IO"
      },
      "outputs": [],
      "source": [
        "# Assuming Y_test_oh is a DataFrame\n",
        "Y_test_oh = convert_to_one_hot(test['sentiment'], 3)\n",
        "print(\"One-Hot Encoded Labels:\")\n",
        "print(Y_test_oh)\n",
        "\n",
        "# Assuming test['sentiment'] is the original sentiment labels\n",
        "print(\"Original Sentiment Labels:\")\n",
        "print(test['sentiment'].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwrqS_JJ_pEZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "test2 = np.array(test['wordseged_space_text'])\n",
        "# Assuming you have test data\n",
        "X_test_indices = sentences_to_indices(test2, word2vec, 128)\n",
        "\n",
        "# One-hot encode the 'sentiment' column\n",
        "Y_test_oh = pd.get_dummies(test['sentiment']).values\n",
        "\n",
        "# Make predictions on the test set\n",
        "Y_pred = model.predict(X_test_indices)\n",
        "\n",
        "# Convert one-hot encoded predictions and true labels back to class labels\n",
        "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
        "Y_true_classes = np.argmax(Y_test_oh, axis=1)\n",
        "rp02 =( Y_true_classes == 0) & (Y_pred_classes == 2)\n",
        "print(test2[rp02])\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(Y_true_classes, Y_pred_classes)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Classification Report with Precision, Recall, and F1-Score\n",
        "class_report = classification_report(Y_true_classes, Y_pred_classes)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xZbH5fRTlLS"
      },
      "outputs": [],
      "source": [
        "np.argmax(preds, axis=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FE_eibix-oOB"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}